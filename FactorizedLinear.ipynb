{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTmd5kmAmLW/Vk/fzotYEv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "2fdmVwnGre8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Gq3mvMFBresi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0YYzo2zuzS_"
      },
      "outputs": [],
      "source": [
        "class FactorizedLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(FactorizedLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        # Factorized weights: W = S * V\n",
        "        self.W = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.S = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.V = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Glorot init for W\n",
        "        nn.init.xavier_uniform_(self.W)\n",
        "        # Normal sampling for weighting\n",
        "        nn.init.normal_(self.S, mean=0.5, std=0.01)\n",
        "        # nn.init.normal_(self.V, mean=,a=0.01)\n",
        "        # Guarantee positive weighting factors with exp\n",
        "        # self.S = nn.Parameter((torch.exp(self.S)))\n",
        "        # Scale V accordingly\n",
        "        # print(\"S: \", self.S.shape)\n",
        "        # print(\"W: \", self.W.shape)\n",
        "        # print(\"V: \", self.V.shape)\n",
        "        s_exp = torch.exp(self.S)\n",
        "        # print(\"s_exp: \", s_exp.shape)\n",
        "        self.V = nn.Parameter(torch.div(self.W, s_exp.view(-1, 1)))\n",
        "\n",
        "\n",
        "        assert torch.isclose(torch.mean(torch.matmul(torch.diag(s_exp), self.V)), torch.mean(self.W))\n",
        "\n",
        "        if self.bias is not None:\n",
        "            nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Compute the full weight matrix W = S * V\n",
        "        s_exp_diag = torch.diag(torch.exp(self.S))\n",
        "        weight = torch.matmul(s_exp_diag, self.V)\n",
        "        # Perform the linear transformation\n",
        "        return F.linear(input, weight, self.bias)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return 'in_features={}, out_features={}, bias={}'.format(\n",
        "            self.in_features, self.out_features, self.bias is not None\n",
        "        )\n",
        "\n",
        "# Example usage:\n",
        "# layer = FactorizedLinear(100, 50)\n",
        "# input_tensor = torch.randn(64, 100) # Batch size 64, input features 100\n",
        "# output_tensor = layer(input_tensor)\n",
        "# print(output_tensor.shape)\n",
        "# print(f\"Number of parameters in full layer: {100 * 50}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = FactorizedLinear(in_features=50, out_features=150)\n",
        "input_tensor = torch.randn(64, 50) # Batch size 64, input features 100\n",
        "output_tensor = layer(input_tensor)\n",
        "print(output_tensor.shape)\n",
        "print(layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQd6qX3m1qOW",
        "outputId": "2f68050b-6f3d-44f1-d889-2fe50847878a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 150])\n",
            "FactorizedLinear(in_features=50, out_features=150, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.Tensor([[1,1,1],[2,2,2]])\n",
        "b = torch.Tensor([[2],[2]])\n",
        "print(b.shape)\n",
        "c = torch.div(a,b)\n",
        "d = torch.matmul(torch.diag(b.flatten()),c)\n",
        "print(c.shape)\n",
        "print(d.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx6ZHce97XUs",
        "outputId": "d6f31f1b-42fa-44c8-a406-898c6e156b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1])\n",
            "torch.Size([2, 3])\n",
            "torch.Size([2, 3])\n"
          ]
        }
      ]
    }
  ]
}